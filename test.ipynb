{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import os\n",
    "import pandas as pd\n",
    "# tf tools\n",
    "import tensorflow as tf\n",
    "# image processsing\n",
    "from tensorflow.keras.preprocessing.image import (load_img,\n",
    "                                                  img_to_array,\n",
    "                                                  ImageDataGenerator)\n",
    "# VGG16 model\n",
    "from tensorflow.keras.applications.vgg16 import (preprocess_input,\n",
    "                                                 decode_predictions,\n",
    "                                                 VGG16)\n",
    "# cifar10 data - 32x32\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "# layers\n",
    "from tensorflow.keras.layers import (Flatten, \n",
    "                                     Dense, \n",
    "                                     Dropout, \n",
    "                                     BatchNormalization)\n",
    "# generic model object\n",
    "from tensorflow.keras.models import Model\n",
    "# optimizers\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "#scikit-learn\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "# for plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # converting the metadata into Pandas objects\n",
    "        # !!! NTS remember to change \"..\" to \"data\" in final version\n",
    "        test_metadata = pd.read_json(os.path.join(\"..\", \"images\", \"metadata\", \"test_data.json\"), lines = True)\n",
    "        train_metadata = pd.read_json(os.path.join(\"..\", \"images\", \"metadata\", \"train_data.json\"), lines = True)\n",
    "        val_metadata = pd.read_json(os.path.join(\"..\", \"images\", \"metadata\", \"val_data.json\"), lines = True)\n",
    "\n",
    "        # !!! (TRY ALSO VALIDATION)\n",
    "        train_metadata = train_metadata.sample(frac=0.01)\n",
    "\n",
    "\n",
    "        # changing the column with the image path from a relative path to an absolute path\n",
    "        test_metadata[\"image_path\"] = \"/work/\" + test_metadata[\"image_path\"]\n",
    "        train_metadata[\"image_path\"] = \"/work/\" + train_metadata[\"image_path\"]\n",
    "        val_metadata[\"image_path\"] = \"/work/\" + val_metadata[\"image_path\"]\n",
    "\n",
    "        # Defining data generater\n",
    "        # flip along x axis (mirror image)\n",
    "        datagen = ImageDataGenerator(horizontal_flip=True, \n",
    "                                    rotation_range=20,\n",
    "                                    validation_split=0.1)\n",
    "\n",
    "        # splitting the data into train, test and validation by using \"flow_from_dataframe\"\n",
    "        # source: code adapted from Kaggle-user vencerlanz09 \n",
    "        # (link to source: https://www.kaggle.com/code/vencerlanz09/indo-fashion-classification-using-efficientnetb0)\n",
    "        BATCH_SIZE = 32\n",
    "        TARGET_SIZE = (224, 224)\n",
    "        train_images = datagen.flow_from_dataframe(\n",
    "            dataframe=train_metadata,\n",
    "            #directory = os.path.join(\"..\", \"images\", \"train\"),\n",
    "            x_col='image_path',\n",
    "            y_col='class_label',\n",
    "            target_size=TARGET_SIZE,\n",
    "            color_mode='rgb',\n",
    "            class_mode='categorical',\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            seed=42,\n",
    "            subset='training')\n",
    "\n",
    "        val_images = datagen.flow_from_dataframe(\n",
    "            dataframe=val_metadata,\n",
    "            #directory = os.path.join(\"..\", \"images\", \"val\"),\n",
    "            x_col='image_path',\n",
    "            y_col='class_label',\n",
    "            target_size=TARGET_SIZE,\n",
    "            color_mode='rgb',\n",
    "            class_mode='categorical',\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            seed=42)\n",
    "\n",
    "        test_images = datagen.flow_from_dataframe(\n",
    "            dataframe=test_metadata,\n",
    "            #directory = os.path.join(\"..\", \"images\", \"test\"),\n",
    "            x_col='image_path',\n",
    "            y_col='class_label',\n",
    "            target_size=TARGET_SIZE,\n",
    "            color_mode='rgb',\n",
    "            class_mode='categorical',\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False)\n",
    "\n",
    "        # assigning labels by getting unique labels from the class column\n",
    "        label_names = test_metadata['class_label'].unique()\n",
    "        \n",
    "        return label_names, train_images, val_images, test_images, test_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    # load model without classifier layers\n",
    "    model = VGG16(include_top=False, \n",
    "                pooling='avg',\n",
    "                input_shape=(32, 32, 3))\n",
    "\n",
    "    # mark loaded layers as not trainable\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # add new classifier layers\n",
    "    flat1 = Flatten()(model.layers[-1].output)\n",
    "    bn = BatchNormalization()(flat1) # NTS : removed one layer\n",
    "    class1 = Dense(128, \n",
    "                activation='relu')(bn)\n",
    "    output = Dense(15, \n",
    "                activation='softmax')(class1) # NTS: changed from 10 to 15 (number of classes)\n",
    "\n",
    "    # define new model\n",
    "    model = Model(inputs=model.inputs, \n",
    "                outputs=output)\n",
    "\n",
    "    # compile\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=0.01,\n",
    "        decay_steps=10000,\n",
    "        decay_rate=0.9)\n",
    "    sgd = SGD(learning_rate=lr_schedule)\n",
    "\n",
    "    model.compile(optimizer=sgd,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_clf(model, train_images, val_images):\n",
    "    H = model.fit(train_images, \n",
    "            validation_data = val_images,\n",
    "            batch_size=128,\n",
    "            epochs=2, # CHANGED FROM 10 FOR TESTING\n",
    "            verbose=1)\n",
    "\n",
    "    return H, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clf(model, train_images, val_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(H, epochs):\n",
    "    plt.style.use(\"seaborn-colorblind\")\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(np.arange(0, epochs), H.history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(np.arange(0, epochs), H.history[\"val_loss\"], label=\"val_loss\", linestyle=\":\")\n",
    "    plt.title(\"Loss curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(np.arange(0, epochs), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "    plt.plot(np.arange(0, epochs), H.history[\"val_accuracy\"], label=\"val_acc\", linestyle=\":\")\n",
    "    plt.title(\"Accuracy curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    # save the plot\n",
    "    plt.savefig(os.path.join(\"out\", \"history_plt.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(H, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_report(model, test_images, test_metadata, label_names):\n",
    "    predictions = model.predict(test_images, batch_size=128)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    y_test = list(test_metadata.class_label) # (new)\n",
    "    clf_report = print(classification_report(y_test, # removed: .argmax(axis=1)\n",
    "                            predictions, # removed: .argmax(axis=1)\n",
    "                            target_names=label_names))\n",
    "    # save the classification report\n",
    "    txtfile_path = os.path.join(\"out\", \"clf_report.txt\")\n",
    "    txtfile = open(txtfile_path, \"w\")\n",
    "    txtfile.write(clf_report)\n",
    "    txtfile.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_report(model, test_images, test_metadata, label_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2 (v3.11.2:878ead1ac1, Feb  7 2023, 10:02:41) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
